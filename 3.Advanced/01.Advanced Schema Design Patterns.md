### Lesson 1: Single Collection Pattern (단일 컬렉션 패턴)

MongoDB의 **Single Collection Pattern**은 관련 데이터를 하나의 컬렉션에 저장하여 읽기 및 쓰기 성능을 최적화하는 방법입니다. 이 패턴은 데이터 구조가 다양하거나, 특정 쿼리를 단일 인덱스 조회로 처리해야 할 때 유용합니다. 아래는 두 가지 변형(Variant)으로 나뉜 패턴을 설명합니다.

---

#### **Variant 1: `docType` 및 `relatedTo` 필드 사용**
1. **설명**:
   - 각 문서에 `docType` 필드를 추가하여 문서 유형(예: 책, 사용자, 리뷰)을 구분합니다.
   - `relatedTo` 필드를 사용해 관련 문서를 연결합니다.
   - 모든 데이터를 하나의 컬렉션(예: `Books Catalog`)으로 이동합니다.

2. **예제 데이터**:
   - 리뷰 문서:
     ```json
     {
        "review_id": 378,
        "docType": "review",
        "relatedTo": [202356]
        ...
     }
     ```

3. **인덱스 생성**:
   - `relatedTo` 배열 필드에 대해 인덱스를 생성하여 관련 문서 쿼리를 지원합니다.
     ```javascript
     db.books_catalog.createIndex({ relatedTo: 1 })
     ```

---

#### **Variant 2: Overloaded `sc_id` 필드 사용**
1. **설명**:
   - `sc_id` 필드를 오버로드하여 문서 간 관계를 표현합니다.
   - 책 문서는 `book_id`를 그대로 사용하고, 리뷰 문서는 `book_id/review_id` 형식으로 저장합니다.
   - 모든 데이터를 하나의 컬렉션(예: `Books Catalog`)으로 이동합니다.

2. **예제 데이터**:
   - 책 문서:
     ```json
     {
        "book_id": 202356,
        "sc_id": "202356"
        ...
     }
     ```
   - 리뷰 문서:
     ```json
     {
        "review_id": 378,
        "sc_id": "202356/378"
        ...
     }
     ```

3. **인덱스 생성**:
   - `sc_id` 필드에 대해 인덱스를 생성하여 관련 문서 쿼리를 지원합니다.
     ```javascript
     db.books_catalog.createIndex({ sc_id: 1 })
     ```

---

### **Beginner-Friendly 추가 설명**

MongoDB를 처음 사용하는 사용자들을 위해 다음과 같은 추가 설명을 제공합니다:

1. **왜 Single Collection Pattern을 사용하는가?**
   - 관계형 데이터베이스와 달리 MongoDB는 조인(join) 연산이 없으므로, 관련 데이터를 한 곳에 저장하면 쿼리 성능이 향상됩니다.
   - 이 패턴은 다양한 구조의 데이터를 단일 컬렉션에 저장할 수 있는 MongoDB의 유연성을 활용합니다.

2. **장점**:
   - 모든 관련 데이터를 한 번의 쿼리로 가져올 수 있습니다.
   - 데이터 구조가 달라도 동일한 컬렉션에 저장 가능하므로 설계가 간단해집니다.

3. **주의사항**:
   - 단일 컬렉션에 너무 많은 데이터를 저장하면 쓰기 충돌이나 삭제 작업의 성능 저하가 발생할 수 있습니다.
   - 적절한 인덱스를 생성하지 않으면 쿼리 성능이 저하될 수 있습니다.

### Lesson 2: Subset Pattern (부분 집합 패턴)

MongoDB의 **Subset Pattern**은 대규모 데이터 배열에서 필요한 데이터의 일부만 저장하거나, 별도의 컬렉션으로 분리하여 관리하는 방법입니다. 이 패턴은 배열 크기가 커질 경우 성능 문제를 해결하고, 데이터 접근성을 향상시키는 데 유용합니다. 아래는 이 패턴을 적용하는 방법을 단계별로 설명합니다.

---

### **Subset Pattern 적용 흐름**

#### **1. Subset Pattern 적용: 리뷰 데이터를 분리**
`books` 컬렉션의 문서에서 `reviews` 배열을 분리하여 새로운 `reviews` 컬렉션에 저장합니다. 이를 위해 MongoDB의 **Aggregation Pipeline**을 사용합니다.

##### **Aggregation Pipeline 코드**
```javascript
db.books.aggregate([
  {
    $unwind: {
      path: "$reviews" // reviews 배열을 개별 문서로 펼침
    }
  },
  {
    $set: {
      "reviews.product_id": "$product_id" // 각 리뷰에 product_id 추가
    }
  },
  {
    $replaceRoot: {
      newRoot: "$reviews" // 리뷰 문서를 최상위 문서로 설정
    }
  },
  {
    $out: "reviews" // 결과를 새로운 reviews 컬렉션에 저장
  }
]);
```

##### **결과**
- `books` 컬렉션에서 `reviews` 배열이 분리되어 `reviews`라는 새로운 컬렉션에 저장됩니다.
- 예를 들어, `books` 컬렉션의 한 문서가 다음과 같다면:
  ```json
  {
    "product_id": 101,
    "title": "MongoDB Guide",
    "reviews": [
      { "review_id": 1, "text": "Great book!", "rating": 5 },
      { "review_id": 2, "text": "Very helpful.", "rating": 4 }
    ]
  }
  ```
- `reviews` 컬렉션에는 다음과 같은 문서가 생성됩니다:
  ```json
  { "review_id": 1, "text": "Great book!", "rating": 5, "product_id": 101 }
  { "review_id": 2, "text": "Very helpful.", "rating": 4, "product_id": 101 }
  ```

---

#### **2. 책 문서의 리뷰 배열 크기 제한**
책 문서(`books` 컬렉션)에 포함된 `reviews` 배열의 크기를 최대 세 개로 제한합니다. 이를 통해 책 문서가 너무 커져 성능이 저하되는 것을 방지합니다.

##### **업데이트 파이프라인**
```javascript
const update_reviews_array_pipeline = [
  {
    $set: {
      reviews: {
        $slice: ["$reviews", 3] // reviews 배열의 첫 세 개 요소만 유지
      }
    }
  }
];

// books 컬렉션 업데이트
db.books.updateMany({}, update_reviews_array_pipeline);
```

##### **결과**
- `books` 컬렉션의 각 문서에서 `reviews` 배열은 최대 세 개의 리뷰만 포함하게 됩니다.
- 예를 들어, 원래 `reviews` 배열이 다섯 개의 리뷰를 포함하고 있었다면:
  ```json
  {
    "product_id": 101,
    "title": "MongoDB Guide",
    "reviews": [
      { "review_id": 1, "text": "Great book!", "rating": 5 },
      { "review_id": 2, "text": "Very helpful.", "rating": 4 },
      { "review_id": 3, "text": "Informative.", "rating": 5 },
      { "review_id": 4, "text": "Good examples.", "rating": 4 },
      { "review_id": 5, "text": "Well-written.", "rating": 5 }
    ]
  }
  ```
- 업데이트 후에는 다음과 같이 변경됩니다:
  ```json
  {
    "product_id": 101,
    "title": "MongoDB Guide",
    "reviews": [
      { "review_id": 1, "text": "Great book!", "rating": 5 },
      { "review_id": 2, "text": "Very helpful.", "rating": 4 },
      { "review_id": 3, "text": "Informative.", "rating": 5 }
    ]
  }
  ```

---

### **Beginner-Friendly 추가 설명**

#### **왜 Subset Pattern을 사용하는가?**
- 데이터 배열이 너무 커지면 읽기 및 쓰기 성능이 저하될 수 있습니다.
- 자주 조회되는 데이터는 원본 문서에 남겨두고, 덜 자주 조회되는 데이터는 별도의 컬렉션으로 분리하면 효율적입니다.

#### **장점**
1. **성능 최적화**:
   - 큰 배열로 인해 발생하는 읽기/쓰기 성능 문제를 해결할 수 있습니다.
2. **데이터 관리 용이성**:
   - 관련 데이터를 별도의 컬렉션으로 분리하여 더 쉽게 관리할 수 있습니다.

#### **주의사항**
- 데이터를 분리하면 쿼리가 복잡해질 수 있습니다(예: 조인 연산 필요).
- 적절한 인덱스를 생성하여 성능 저하를 방지해야 합니다.

---

### **실습 예제**

#### **1. 데이터 삽입**
먼저 `books` 컬렉션에 데이터를 삽입합니다.
```javascript
db.books.insertMany([
  {
    product_id: 101,
    title: "MongoDB Guide",
    reviews: [
      { review_id: 1, text: "Great book!", rating: 5 },
      { review_id: 2, text: "Very helpful.", rating: 4 },
      { review_id: 3, text: "Informative.", rating: 5 },
      { review_id: 4, text: "Good examples.", rating: 4 },
      { review_id: 5, text: "Well-written.", rating: 5 }
    ]
  }
]);
```

#### **2. Aggregation Pipeline 실행**
위에서 제공한 Aggregation Pipeline을 실행하여 `reviews` 컬렉션을 생성합니다.
```javascript
db.books.aggregate([
  { $unwind: { path: "$reviews" } },
  { $set: { "reviews.product_id": "$product_id" } },
  { $replaceRoot: { newRoot: "$reviews" } },
  { $out: "reviews" }
]);
```

#### **3. 책 문서 업데이트**
책 문서의 `reviews` 배열 크기를 제한합니다.
```javascript
const update_reviews_array_pipeline = [
   {
     $set: {
       reviews: {
         $slice: ["$reviews",3]
       }
     }
   }
];

db.books.updateMany({}, update_reviews_array_pipeline);
```

#### **4. 결과 확인**
- `books`와 `reviews` 두 컬렉션을 각각 조회하여 데이터를 확인합니다.
```javascript
// books 컬렉션 확인
db.books.find().pretty();

// reviews 컬렉션 확인
db.reviews.find().pretty();
```

---

### Lesson 3: Bucket Pattern (버킷 패턴)

MongoDB의 **Bucket Pattern**은 대량의 데이터를 시간, 범위, 또는 다른 기준으로 그룹화하여 저장하는 방법입니다. 이 패턴은 데이터의 크기를 줄이고, 읽기 성능을 최적화하며, 특정 시점 또는 범위에 대한 데이터를 효율적으로 조회할 수 있도록 설계되었습니다. 아래는 이 패턴을 적용하는 방법과 예제를 단계별로 설명합니다.

---

### **Bucket Pattern 적용 흐름**

#### **1. Bucket Document 생성**
버킷 패턴을 적용하기 위해, 특정 기준(예: 책 ID와 월별 타임스탬프)에 따라 데이터를 그룹화하여 하나의 버킷 문서로 저장합니다.

##### **예제 버킷 문서**
```json
{
  "id": {
    "book_id": 1,
    "month": {
      "$date": "2023-11-01T00:00:00.000Z"
    }
  },
  "views": [
    {
      "user_id": "user00",
      "timestamp": {
        "$date": "2023-11-22T00:00:00.000Z"
      }
    },
    {
      "user_id": "user10",
      "timestamp": {
        "$date": "2023-11-22T00:01:00.000Z"
      }
    }
  ]
}
```

- `id` 필드는 버킷의 고유 식별자로, `book_id`와 `month`를 조합하여 생성합니다.
- `views` 배열에는 해당 월 동안 발생한 모든 조회(view) 데이터를 저장합니다.

---

#### **2. 기존 컬렉션에서 Bucket Pattern 생성**
기존 `views` 컬렉션이 개별 조회 데이터를 저장하고 있다면, Aggregation Pipeline을 사용해 데이터를 버킷으로 그룹화할 수 있습니다.

##### **기존 `views` 컬렉션 예제 데이터**
```json
{
  "book_id": 34538756,
  "timestamp": {
    "$date": "2023-09-29T08:23:13Z"
  },
  "user_id": 271828
}
```

##### **Aggregation Pipeline 코드**
다음 파이프라인은 `views` 컬렉션의 데이터를 월별로 그룹화하여 새로운 버킷 문서를 생성합니다.
```javascript
db.views.aggregate([
  {
    $group: {
      _id: {
        book_id: "$book_id",
        month: {
          $dateFromParts: {
            year: { $year: "$timestamp" },
            month: { $month: "$timestamp" },
            day: 1 // 월의 첫 번째 날로 설정
          }
        }
      },
      views: {
        $push: {
          user_id: "$user_id",
          timestamp: "$timestamp"
        }
      }
    }
  },
  {
    $set: {
      views_count: { $size: "$views" } // 조회 수 추가
    }
  }
]);
```

##### **결과**
위 파이프라인 실행 후 생성되는 문서는 다음과 같습니다:
```json
{
  "_id": {
    "book_id": 34538756,
    "month": {
      "$date": "2023-09-01T00:00:00Z"
    }
  },
  "views": [
    {
      "user_id": 271828,
      "timestamp": {
        "$date": "2023-09-29T08:23:13Z"
      }
    },
    ...
  ],
  "views_count": 10
}
```

---

### **Beginner-Friendly 추가 설명**

#### **왜 Bucket Pattern을 사용하는가?**
1. **데이터 크기 최적화**:
   - 개별 문서를 하나씩 저장하는 대신, 관련 데이터를 하나의 버킷에 묶어 저장하면 스토리지 사용량을 줄일 수 있습니다.
2. **읽기 성능 향상**:
   - 한 번의 쿼리로 관련 데이터를 모두 가져올 수 있어 읽기 성능이 향상됩니다.
3. **시간 기반 분석**:
   - 시간(예: 월별) 기준으로 데이터를 그룹화하면 분석 작업이 더 쉬워집니다.

#### **장점**
- 데이터가 논리적으로 그룹화되어 관리가 용이합니다.
- 특정 범위(예: 특정 월)의 데이터를 빠르게 검색할 수 있습니다.

#### **주의사항**
- 데이터가 너무 커지지 않도록 적절한 기준(예: 월 단위)으로 버킷을 설계해야 합니다.
- 쓰기 작업 시, 동일한 버킷에 여러 클라이언트가 동시에 접근하면 충돌 가능성이 있으므로 주의해야 합니다.

---

### **실습 예제**

#### **1. 데이터 삽입**
먼저 `views` 컬렉션에 개별 조회 데이터를 삽입합니다.
```javascript
db.views.insertMany([
  { book_id: 1, timestamp: new Date("2023-11-22T00:00:00Z"), user_id: "user00" },
  { book_id: 1, timestamp: new Date("2023-11-22T00:01:00Z"), user_id: "user10" },
  { book_id: 1, timestamp: new Date("2023-11-23T12:30:00Z"), user_id: "user20" },
  { book_id: 2, timestamp: new Date("2023-11-22T15:45:00Z"), user_id: "user30" }
]);
```

#### **2. Aggregation Pipeline 실행**
위에서 제공한 Aggregation Pipeline을 실행하여 데이터를 월별로 그룹화하고 새로운 컬렉션(예:`buckets`)에 저장합니다.
```javascript
db.views.aggregate([
  {
    $group: {
      _id: {
        book_id: "$book_id",
        month: {
          $dateFromParts: {
            year: { $year: "$timestamp" },
            month: { $month: "$timestamp" },
            day: 1
          }
        }
      },
      views: {
        $push: {
          user_id: "$user_id",
          timestamp: "$timestamp"
        }
      }
    }
  },
  {
    $set: { views_count: { $size: "$views" } } // 조회 수 추가
  },
  {
    $out: "buckets" // 결과를 buckets 컬렉션에 저장
  }
]);
```

#### **3. 결과 확인**
새롭게 생성된 `buckets` 컬렉션을 확인합니다.
```javascript
db.buckets.find().pretty();
```

##### **결과 예시**
```json
{
  "_id": { 
    "book_id": 1,
    "month": ISODate("2023-11-01T00:00:00Z")
   },
   "views_count": 3,
   "views": [
     { user_id:"user00", timestamp : ISODate("2023-11-22T00 :01 :00")},
     ...
   ]
}
```

---

### Lesson 4: Outlier Pattern (이상치 패턴)

MongoDB의 **Outlier Pattern**은 데이터에서 특정 기준을 초과하는 "이상치(outliers)"를 식별하고, 이를 별도로 관리하거나 태그를 추가하여 처리하는 방법입니다. 이 패턴은 데이터 크기가 비정상적으로 큰 문서를 효율적으로 관리하거나, 쿼리 성능을 최적화하기 위해 사용됩니다. 아래는 이 패턴을 적용하는 방법과 예제를 단계별로 설명합니다.

---

### **Outlier Pattern 적용 흐름**

#### **1. Outlier 필드 추가**
리뷰 문서에서 `comments` 배열의 크기가 3개를 초과하는 경우, 해당 문서를 "이상치"로 간주하고 `outlier` 필드를 추가합니다. 이 필드는 `true`로 설정됩니다.

##### **예제 문서**
- **업데이트 전**:
```json
{
  "rating": 1,
  "userId": 318,
  "productId": 34538756,
  "review_text": "Lorem ipsum dolor sit amet...",
  "comments": [
    { "userId": 101, "text": "Great review!" },
    { "userId": 102, "text": "I disagree." },
    { "userId": 103, "text": "Very detailed." },
    { "userId": 104, "text": "Thanks for sharing!" }
  ]
}
```

- **업데이트 후**:
```json
{
  "rating": 1,
  "userId": 318,
  "productId": 34538756,
  "review_text": "Lorem ipsum dolor sit amet...",
  "comments": [
    { "userId": 101, "text": "Great review!" },
    { "userId": 102, "text": "I disagree." },
    { "userId": 103, "text": "Very detailed." },
    { "userId": 104, "text": "Thanks for sharing!" }
  ],
  "outlier": true
}
```

---

#### **2. 기존 문서 업데이트**
기존 리뷰 문서에서 `comments` 배열의 크기가 **3개를 초과**하는 문서를 찾아 `outlier` 필드를 추가합니다.

##### **업데이트 코드**
```javascript
db.reviews.updateMany(
    {
        // comments 배열에 네 번째 요소가 존재하는 문서를 찾음
        "comments.3": { $exists: true }
    },
    {
        $set: { outlier: true } // outlier 필드를 추가하고 true로 설정
    }
);
```

##### **결과**
- `comments` 배열이 세 개 이하인 문서는 변경되지 않습니다.
- `comments` 배열이 네 개 이상인 문서에는 `outlier: true` 필드가 추가됩니다.

---

### **Beginner-Friendly 추가 설명**

#### **왜 Outlier Pattern을 사용하는가?**
1. **데이터 크기 관리**:
   - 데이터 크기가 비정상적으로 큰 문서를 식별하고 별도로 관리하면 컬렉션의 균형을 유지할 수 있습니다.
2. **쿼리 성능 최적화**:
   - 이상치 데이터를 따로 처리하거나 태그를 추가하면 쿼리 성능을 개선할 수 있습니다.
3. **유지보수 용이성**:
   - 이상치를 명시적으로 태그하여 나중에 쉽게 추적하거나 분석할 수 있습니다.

#### **장점**
- 이상치 데이터를 효율적으로 관리할 수 있습니다.
- 데이터 구조의 일관성을 유지하면서도 특정 조건에 맞는 데이터를 식별할 수 있습니다.

#### **주의사항**
- 이상치 기준(예: 배열 크기)을 적절히 설정해야 합니다.
- 너무 많은 문서가 이상치로 분류되면 패턴의 효과가 감소할 수 있습니다.

---

### **실습 예제**

#### **1. 데이터 삽입**
먼저 `reviews` 컬렉션에 데이터를 삽입합니다.
```javascript
db.reviews.insertMany([
  {
    rating: 5,
    userId: 101,
    productId: 12345,
    review_text: "Amazing book!",
    comments: [
      { userId: 201, text: "I agree!" },
      { userId: 202, text: "Very helpful." }
    ]
  },
  {
    rating: 1,
    userId: 318,
    productId: 34538756,
    review_text: "Lorem ipsum dolor sit amet...",
    comments: [
      { userId: 101, text: "Great review!" },
      { userId: 102, text: "I disagree." },
      { userId: 103, text: "Very detailed." },
      { userId: 104, text: "Thanks for sharing!" }
    ]
  }
]);
```

#### **2. Outlier 필드 업데이트**
위에서 제공한 코드를 실행하여 `comments` 배열 크기가 세 개를 초과하는 문서에 `outlier` 필드를 추가합니다.
```javascript
db.reviews.updateMany(
    {
        // comments 배열에 네 번째 요소가 존재하는 문서를 찾음
        "comments.3": { $exists: true }
    },
    {
        $set: { outlier: true } // outlier 필드를 추가하고 true로 설정
    }
);
```

#### **3. 결과 확인**
업데이트된 `reviews` 컬렉션을 확인합니다.
```javascript
db.reviews.find().pretty();
```

##### **결과 예시**
- `comments` 배열이 세 개 이하인 문서는 변경되지 않습니다:
```json
{
  "_id": ObjectId("..."),
  "rating": 5,
  "userId": 101,
  "productId": 12345,
  "review_text": "Amazing book!",
  "comments": [
    { userId:201,text:"I agree!"},
     ...
   ]
}
```
- `comments` 배열이 네 개 이상인 문서에는 `outlier` 필드가 추가됩니다:
```json
{
   "_id" : ObjectID("...."),
"outlier:true"
....
}
```

### Lesson 5: Archive Pattern (아카이브 패턴)

MongoDB의 **Archive Pattern**은 오래되거나 자주 조회되지 않는 데이터를 별도로 저장하여 데이터베이스 성능을 최적화하고 스토리지 비용을 절감하는 방법입니다. 이 패턴은 데이터 수명이 긴 애플리케이션에서 데이터 관리와 성능 유지에 특히 유용합니다. 아래는 이 패턴을 적용하는 방법과 예제를 단계별로 설명합니다.

---

### **Archive Pattern 적용 흐름**

#### **1. 아카이브 데이터 식별**
먼저, 오래되거나 자주 사용되지 않는 데이터를 식별합니다. 예를 들어, `reviews` 컬렉션에서 특정 날짜 이전의 데이터를 아카이브 대상으로 지정할 수 있습니다.

##### **예제 문서**
- **아카이브 전**:
```json
{
  "review_id": 123,
  "productId": 34538756,
  "review_text": "Great product!",
  "timestamp": {
    "$date": "2023-01-15T08:23:13Z"
  }
}
```

---

#### **2. 아카이브 데이터 이동**
아카이브 대상 데이터를 별도의 저장소(예: 다른 MongoDB 컬렉션, 클라우드 스토리지)로 이동합니다.

##### **Aggregation Pipeline을 사용한 데이터 이동**
아래 코드는 특정 조건(예: `timestamp`가 특정 날짜 이전인 문서)을 만족하는 데이터를 아카이브 컬렉션으로 이동합니다.
```javascript
db.reviews.aggregate([
  {
    $match: { 
      timestamp: { $lt: new Date("2024-01-01") } // 특정 날짜 이전의 데이터 필터링
    }
  },
  {
    $out: "archived_reviews" // 데이터를 archived_reviews 컬렉션으로 이동
  }
]);
```

##### **결과**
- `reviews` 컬렉션에서 필터링된 데이터가 `archived_reviews` 컬렉션으로 복사됩니다.

---

#### **3. 원본 데이터 삭제**
아카이브된 데이터를 원본 컬렉션에서 삭제하여 스토리지를 최적화합니다.
```javascript
db.reviews.deleteMany({
  timestamp: { $lt: new Date("2024-01-01") }
});
```

---

#### **4. MongoDB Atlas Online Archive 활용 (선택 사항)**
MongoDB Atlas의 Online Archive 기능을 사용하면 자동으로 오래된 데이터를 클라우드 스토리지로 이동할 수 있습니다.

##### **Online Archive 설정 예시**
1. MongoDB Atlas UI에서 Online Archive를 활성화합니다.
2. 아카이브 규칙을 설정합니다(예: `timestamp` 필드 기준으로 1년 이상 된 데이터를 이동).
3. 설정 후, Atlas는 자동으로 데이터를 클라우드 스토리지(Amazon S3 등)로 이동합니다.

---

### **Beginner-Friendly 추가 설명**

#### **왜 Archive Pattern을 사용하는가?**
1. **성능 최적화**:
   - 자주 사용되지 않는 데이터를 분리하여 쿼리 성능을 향상시킵니다.
2. **스토리지 비용 절감**:
   - 덜 중요한 데이터를 저렴한 스토리지로 옮겨 비용을 줄입니다.
3. **데이터 수명 관리**:
   - 오래된 데이터를 체계적으로 관리하여 데이터베이스를 깔끔하게 유지할 수 있습니다.

#### **장점**
- 프로덕션 데이터베이스의 부하를 줄여 성능을 유지합니다.
- 데이터 접근성을 유지하면서도 비용 효율적인 저장소를 활용할 수 있습니다.

#### **주의사항**
- 아카이브된 데이터에 접근해야 할 경우, 별도의 쿼리 전략이나 복구 계획이 필요합니다.
- 아카이브 대상 데이터를 정확히 식별하는 것이 중요합니다.

---

### **실습 예제**

#### **1. 데이터 삽입**
먼저 `reviews` 컬렉션에 테스트 데이터를 삽입합니다.
```javascript
db.reviews.insertMany([
  { review_id: 1, productId: 101, review_text: "Great!", timestamp: new Date("2023-01-15") },
  { review_id: 2, productId: 102, review_text: "Not bad.", timestamp: new Date("2024-02-20") },
  { review_id: 3, productId: 103, review_text: "Could be better.", timestamp: new Date("2022-11-10") }
]);
```

#### **2. 아카이브 대상 필터링 및 이동**
Aggregation Pipeline을 사용하여 특정 날짜 이전의 리뷰를 `archived_reviews` 컬렉션으로 이동합니다.
```javascript
db.reviews.aggregate([
  {
    $match: { 
      timestamp: { $lt: new Date("2024-01-01") } // 특정 날짜 이전의 리뷰 선택
    }
  },
  {
    $out: "archived_reviews" // archived_reviews 컬렉션으로 이동
  }
]);
```

#### **3. 원본 데이터 삭제**
아카이브된 데이터를 원본 `reviews` 컬렉션에서 삭제합니다.
```javascript
db.reviews.deleteMany({
  timestamp: { $lt: new Date("2024-01-01") }
});
```

#### **4. 결과 확인**
각 컬렉션에서 데이터를 확인하여 아카이브가 제대로 수행되었는지 확인합니다.
```javascript
// 원본 컬렉션 확인
db.reviews.find().pretty();

// 아카이브 컬렉션 확인
db.archived_reviews.find().pretty();
```

---

### **추가 팁**
1. MongoDB Compass 또는 Atlas UI를 사용하면 아카이브 프로세스를 시각적으로 관리할 수 있습니다.
2. TTL(Time-To-Live) 인덱스를 설정하면 특정 시간이 지난 문서를 자동으로 삭제하거나 이동할 수 있습니다:
   ```javascript
   db.reviews.createIndex({ timestamp: 1 }, { expireAfterSeconds: 31536000 }); // 1년 후 자동 삭제
   ```


# The recommended maximum number of collections are as follows: 5,000 per M10, 10,000 per M20/M30, 10,000 per replica set, and 10,000 per collection.

